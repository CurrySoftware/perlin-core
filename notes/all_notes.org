* Requirements
** Speed
*** Any Query on 100MB indexed rawdata should execute in < 10ms
*** Simple (< 4 terms, no phrase query) Queries should execute in < 1ms
** Memory 
*** Minimal Memory Overhead per Index enabeling multi-tennant Systems
*** Moving data from memory to disk or network and back should be seamlessly possible depending on ram/disk pressure
*** Allow shared dictionaries between different indices for minimal memory overhead
** Usability
*** sensible defaults, which can be tweeked if needed
*** documentation of public api
*** as few as possible generic parameters on index
** Flexibility
*** data-type independent
*** plug-in your own
**** Compression Algorithm
**** Query Operator and Evaluator
**** Data Storage mechanism
**** Scoring/Ranking Method
** Functionality
*** Mutable Indices
*** Boolean Retrieval
*** Ranked Retrieval
*** Semantic Relations (Taxonomies, Thesaurus, etc.)
*** Relevance Feedback


* Modules
** Current Status
*** Index
**** Vocabulary
**** InvertedIndex
**** Query Engine
*** Compression
**** General Purpose Compression
**** Integer Compression
*** PageManager
**** RamPageManager
**** DiskPageManager
**** NetworkPageManager



* Processes
** Index documents
** Execute Query
*** QueryAtom
#+begin_src plantuml :file query_atom.png
participant Index
participant Vocabulary
participant InvertedIndex
participant QueryExecutor

[-> Index : execute_query(atom)
Index -> Vocabulary : get_term_id(atom)
Vocabulary -> Index : term_id
Index -> InvertedIndex : get_iterator(term_id)
InvertedIndex -> Index : PostingIterator
Index -> QueryExecutor : new(PostingIterator, deletedDocuments)
QueryExecutor -> Index : Self
Index ->[ : QueryExecutor

#+end_src

#+RESULTS:
[[file:query_atom.png]]
*** ArbitraryQuery
#+begin_src plantuml :file arbitrary_query.png
[-> Index : execute_query(Operator, Operands)
Index -> Vocabulary : resolve_operands(Operands)

#+end_src
** Pages Blocks
*** Indexing Process
#+begin_src plantuml :file indexing_process.png
title DEPRECATED! see Adding to DiskList
participant Indexer
participant Listing
participant Compressor
database RamPageManager
database FsPageManager
database NwPageManager
== First Page ==
Indexer -> Listing : add(&[Posting])
Listing -> Compressor : compress(&[Posting])
Compressor -> Listing : Block
Listing -> RamPageManager : store_new(block)
RamPageManager -> Listing : page_id
== In Page ==
Indexer -> Listing : add(&[Posting])
Listing -> Compressor : compress(&[Posting])
Compressor -> Listing : Block
Listing -> RamPageManager : store_in_place(block, page_id, block_id)
RamPageManager -> Listing : Ok(block_id)
== Page Full ==
Indexer -> Listing : add(&[Posting])
Listing -> Compressor : compress(&[Posting])
Compressor -> Listing : Block
Listing -> RamPageManager : store_in_page(block, page_id)
RamPageManager -> FsPageManager : store_page(page_id, Page)
FsPageManager -> NwPageManager : store_page(page_id, Page)
NwPageManager -> FsPageManager : Ok
FsPageManager -> RamPageManager : Ok
RamPageManager -> Listing : Err(page_id)
== Commit Unfull Page == 
Indexer -> Listing : commit()
Listing -> RamPageManager : commit_page(page_id)
RamPageManager -> FsPageManager : store_page(page_id, Page)
FsPageManager -> RamPageManager : Ok
RamPageManager -> Listing : Ok
Listing -> Indexer : Ok
#+end_src

#+RESULTS:
[[file:indexing_process.png]]
*** Document Deletion
#+begin_src plantuml :file document_deletion.png
title Docstorage Concept
participant Index
participant Listing
participant DocStore
participant BlockIterator
participant RamPageManager
participant FsPageManager
participant NwPageManager

[-> Index : delete_document(doc_id)
Index -> DocStore : get_listings(doc_id)
DocStore -> Index : &[term_ids]
Index -> Listing : delete_posting(doc_id)
Listing -> Listing : delete_buffer_full()?
Listing -> BlockIterator : get_all(&[(page_id, block_id)])
BlockIterator -> Listing : &[Blocks]
Listing -> Listing : DecodeRemoveEncode
  loop over new pages
    Listing -> RamPageManager : store_page(Page) 
    RamPageManager -> FsPageManager : store_page(Page)
    FsPageManager -> RamPageManager : Ok(page_id)
    RamPageManager -> Listing : Ok(page_id)    
  end
Listing -> Index : NewBlockList
Index -> Listing : UpdateBlockList(NewBlockList)
loop over old pages
Listing -> RamPageManager : delete_page(page_id)
RamPageManager -> FsPageManager : delete_page(page_id)
FsPageManager -> RamPageManager : Ok()
RamPageManager -> Listing : Ok()
end
Listing -> Index : Ok()
#+end_src

#+RESULTS:
[[file:document_deletion.png]]

*** Incremental Indexing
#+begin_src plantuml file: incremental_indexing.png
participant Listing
participant RamPageCache
database FsPagemanager
[-> Listing : add(&[Posting])
Listing -> RamPageCache : get_page(pages.unfull)
RamPageCache -> Listing : Page
Listing -> RamPageCache : delete_unfull(pages.unfull)
Listing -> RamPageCache : store_blocks
Listing -> 
#+end_src
*** Get Page/Block
#+begin_src plantuml :file fetch_block.png
 participant BlockIterator
 database RamPageManager
 database FsPageManager
 database NwPageManager
 BlockIterator -> RamPageManager : get(page_id)
 RamPageManager -> FsPageManager : get(page_id)
 FsPageManager -> NwPageManager : get(page_id)

 NwPageManager --> FsPageManager : Page
 FsPageManager --> RamPageManager : Page
 RamPageManager --> BlockIterator : Page
 #+end_src

 #+RESULTS:
 [[file:fetch_block.png]]
*** Iterate over Blocks
#+begin_src plantuml :file iterate_blocks.png
start
  if (!Page Available?)    
  else
    -get_page(page_id)
  endif
  -yield page[block]
end
#+end_src

#+RESULTS:
[[file:iterate_blocks.png]]
*** Adding to DiskList 
#+begin_src plantuml :file disk_list.png
participant DiskList
participant RamPageCache
database FsPageManager
== First Block ==
DiskList -> RamPageCache : store_block(block)
RamPageCache -> DiskList : PageId

== Second Block == 
DiskList -> RamPageCache : store_in_place(block, page_id, block_id)
RamPageCache -> DiskList : Ok()
== Full Page ==
DiskList -> RamPageCache : store_in_place(block, page_id, block_id)
RamPageCache -> DiskList : Ok()
DiskList -> RamPageCache : commit_page(page_id)
RamPageCache -> FsPageManager : store_full(Page) 
FsPageManager -> RamPageCache : PageId
RamPageCache -> DiskList : PageId 
== Commit Unfull Page ==
DiskList -> RamPageCache : commit_unfull_page(page_id, block_id)
RamPageCache -> FsPageManager : store_unfull_page(Page, block_id)
FsPageManager -> RamPageCache : UnfullPage(PageId, BlockId, BlockId)
RamPageCache -> DiskList : UnfullPage(PageId, BlockId, BlockId)
#+end_src

#+RESULTS:
[[file:disk_list.png]]

*** Removing From DiskList 
#+begin_src plantuml :file remove_disk_list.png
participant DiskList
participant RamPageCache
database FsPageManager

#+end_src
*** Removing UnfullPage
#+begin_src plantuml :file remove_unfull_page.png
participant RamPageCache
participant FsPageManager
database FileSystem
[-> RamPageCache : delete_unfull(PageId) 
RamPageCache -> FsPageManager : delete_unfull(PageId)
FsPageManager -> FileSystem : decdement first byte of PageId
FileSystem -> FsPageManager : refcount
FsPageManager -> FsPageManager : if refcount == 0 Add PageId to VacantPages
FsPageManager -> RamPageCache : Ok()
[<- RamPageCache : Ok()
#+end_src

#+RESULTS:
[[file:remove_unfull_page.png]]

** Compression
*** Compress to Block
*** Delta Compression
#+begin_src plantuml :file delta_compression.png
participant Listing
participant Compressor
database RamPageCache
[-> Listing : add(&[Posting])
Listing -> Listing : set_block_end(DocId)
Listing -> Compressor : compress(RingBuffer<Posting>)
Compressor -> Listing : Block
Listing -> RamPageCache : store_block(block)
RamPageCache -> Listing : PageId
Listing -> Listing : store(PageId, BlockId, BlockStart)
Listing -> Listing : set block_start = block_end
#+end_src
#+RESULTS:
[[file:delta_compression.png]]

#+begin_src plantuml :file based_ringbuffer.png


#+end_src

** Fields
*** Indexing
#+begin_src plantuml :file field_indexing.png
participant PerlinIndex
participant Counter
participant TitleIndex
participant ContentIndex
participant DateIndex
[-> PerlinIndex : index_document(title, content, date)
PerlinIndex -> Counter : next_doc_id()
Counter -> PerlinIndex : DocId
PerlinIndex -> TitleIndex : index_document(title, doc_id)
PerlinIndex -> ContentIndex : index_document(content, doc_id)
PerlinIndex -> DateIndex : index_document(date, doc_id)
[<- PerlinIndex : doc_id
#+end_src

#+RESULTS:
[[file:field_indexing.png]]
*** Querying...
#+begin_src plantuml :file field_querying

#+end_src
** OrdFields
*** Continous
The idea here is to map continous variable intervalls as discrete values.
That enables incomplete query execution, which in turn allows fast query execution!

**** Sorting
#+begin_src plantuml :file continous_field_sorting.png
title Sort query result by date
participant PerlinIndex
participant ContentIndex
participant DateIndex
participant QueryEngine
[-> PerlinIndex : "x and y order by date"
PerlinIndex -> ContentIndex : Query(A + B)
ContentIndex -> PerlinIndex : A+B Query
PerlinIndex -> DateIndex : Query(DateIntervall 1)
DateIndex -> PerlinIndex : DateIntervall 1 Query
PerlinIndex -> QueryEngine : run join A+B Query & DateIntervall 1 Query
QueryEngine -> PerlinIndex : Postings
== If Postings.len() < (page+1)*pagesize ==
PerlinIndex -> DateIndex : Query(DateIntervall 2)
DateIndex -> PerlinIndex : DateIntervall 2 Query
PerlinIndex -> QueryEngine : run join A+B Query & DateIntervall 2 Query
QueryEngine -> PerlinIndex : Postings
== End if ==
[<- PerlinIndex: Postings.sort().skip(page*pagesize).take(pagesize)
#+end_src

#+RESULTS:
[[file:continous_field_sorting.png]]

*** Discrete
**** Implementation
**** Faceting

* Ownership
** RamPageCache
#+begin_src plantuml :file ownership_rampagecache.png
object Index
object RamPageCache
object Listing1
object Listing2
object BlockIter1
object BlockIter2

Index -|> RamPageCache
Index -|> Listing1
Index -|> Listing2
BlockIter1 ..> RamPageCache
BlockIter2 ..> RamPageCache
#+end_src

#+RESULTS:

[[file:ownership_rampagecache.png]]
* Traits
** Vocabulary
*** Method
fn get_or_add(Term) -> term_id
** PageManager
*** Methods
fn store_page(Page) -> page_id
fn get_blocks(Vec<(page_id, block_id)>) -> impl Iterator<Item=Block>
fn store_new_block(Block) -> page_id
fn store_in_page(page_id, Block) -> Result<block_id, page_id>
** Compressable
*** Methods
fn compress(&mut RingBuffer<Self>) -> Result<Block>
fn decompress(Block, &mut RingBuffer<Self>) -> Result<usize>


* Implementation Plans
** General Strategy
Find modular testable things. Implement them module by module with extensive tests for all
** Modules
*** DONE RingBuffer
    CLOSED: [2016-12-19 Mon 20:23]
1. Implement Generically
*** DONE PageManager
    CLOSED: [2016-12-20 Tue 19:53]
**** DONE Define Trait 
     CLOSED: [2016-12-19 Mon 20:24]
**** DONE Implement RamPageManager
     CLOSED: [2016-12-19 Mon 20:24]
**** DONE Implement FsPageManager
     CLOSED: [2016-12-20 Tue 11:16]
**** DONE Implement Combination
     CLOSED: [2016-12-20 Tue 13:31]
**** DONE Implement BlockIterator
     CLOSED: [2016-12-20 Tue 17:11]
*** DONE Listings and Postings
    CLOSED: [2016-12-20 Tue 19:53]
1. Define Postings and Listings
2. Implement naive compression algorithm
*** DONE Vocabulary
    CLOSED: [2016-12-21 Wed 15:00]
1. Define Vocabulary Trait
2. Implement Vocabulary Trait for HashMap
*** DONE Index
    CLOSED: [2016-12-22 Thu 13:54]
1. Define Struct
2. Implement Indexing


* Filter
** TopK most relevant filters
*** Problem Definition
Dimensions:
Number Filters: 10.000
Average Number Documents that match Filter: 10.000
Max Time: 1ms
=> 100 ns per Filter

Sorting 10.000 values takes about 0.5ms!

One comparison takes !at least! (w/o disk IO) 2ns!
Leaves us with about 500.000 operations if we dont sort and 250.000 operations if we sort!

Data:
Query Result:
&[DocId]

Filter#1
&[DocId]

Filter#2
&[DocId]

Filter 1 is more relevant when |Filter#1 intersected with query result| > |Filter#2 intersected with query result|

*** Naive Approach
for each f in filters
calculate |f intersected with query result|

sort filters by set_size
take top k

Estimation for worst case:
10.000 intersections * 10.000 operations => 100.000.000 Operations
in 1.000.000 ns

Or O(|F| * min(|QR|, |aF|)) 

about 100x too slow!

*** Sampling Approach
Idea: Only sample a fraction of the whole result set, project the resulting set size from this sample data
Number of Samples per Filter = 10
Samplesize = 16

if filter.size() > 200 sample
else
count

10.000 * 160 => 1.600.000
in 1.000.000 ns

about 10x too slow!

*** Sampling + Throw away impossibles
Idea: Sample large filters, count small ones, dont do anything with ones that are smaller than current limit!
Current limit is ResultSize[k]!

Need the information of DocumentFrequency for that to work.

Pseudocode

TopKFilterOptions: &[FilterOption] -> ResultSet -> &[(FilterOption, usize)]
results: &[(FilterOption, usize)]
for option in options {
  if option.size() > limit {
     let size = sample_intersect(option, result_set).size();
     if size > limit {
        results.insert(index, (option, size));
        limit = results[k].1;
     }
  }
}
